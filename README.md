# ğŸ§  RNN â€” From Scratch with Python + NumPy

A handsâ€‘on tutorial-driven project to build your first **Recurrent Neural Network (RNN)** from scratch using Python and NumPy. This RNN performs **character-level language classification**, predicting the language origin of names (e.g., detecting if "Rodrigo" is Portuguese).

---

## ğŸš€ Project Overview

- **Objective**: Implement a simple RNN from first principlesâ€”without high-level frameworksâ€”to understand its inner workings (forward pass, hidden state, and BPTT).
- **Use Case**: Character-level classification: reads a person's name letter by letter and predicts their language of origin.

---

## ğŸ“¦ Repository Structure
```
â”œâ”€â”€ data/ # Sample name lists per language
â”œâ”€â”€ models/ # Saved model parameters (optional)
â”œâ”€â”€ notebooks/ # Jupyter notebooks for experimentation
â”‚ â”œâ”€â”€ explore.ipynb # Data analysis & visualization
â”‚ â””â”€â”€ rnn_implementation.ipynb # Step-by-step RNN build
â”œâ”€â”€ src/
â”‚ â”œâ”€â”€ rnn.py # Core RNN implementation (forward, backprop)
â”‚ â”œâ”€â”€ utils.py # Data loading, encoding, helper functions
â”‚ â””â”€â”€ train.py # Training loop using SGD and BPTT
â”œâ”€â”€ requirements.txt # Project dependencies
â””â”€â”€ README.md # Project overview and instructions
```

---

## ğŸ› ï¸ Key Features

- **Character embeddings**: One-hot encoded characters as input vectors.
- **Hidden state propagation**: Maintains and updates hidden layer across time steps.
- **Back-Propagation Through Time (BPTT)**: Manual implementation of gradient propagation over sequences.
- **Cross-entropy loss**: Used with softmax over character predictions.
- **Training loop**: Mini-batch SGD, learning rate decay, and loss tracking.
- **Evaluation**: Accuracy on test examples and confusion matrix to analyze model performance.

---

## âš™ï¸ Installation

```bash
git clone https://github.com/saranjthilak/data-your-first-rnn.git
cd data-your-first-rnn
python3 -m venv venv
source venv/bin/activate
pip install -r requirements.txt
```
## ğŸ”„ Usage
1. Prepare Data
Place your .txt files for each language in the data/ folder (e.g. English.txt, Italian.txt).

2. Train the Model
```bash
Copy
Edit
python src/train.py \
  --data_dir data/ \
  --hidden_dim 128 \
  --learning_rate 0.005 \
  --epochs 10
```
3. Monitor Training
Check the loss curve and accuracy output:
```
python-repl
Copy
Edit
Epoch 1/10 â€” Loss: 3.12 â€” Accuracy: 12.5%
```
4. Test & Evaluate
```bash
Copy
Edit
python src/train.py --mode test --model_path models/rnn_epoch10.pkl
Generates metrics and a confusion matrix to assess performance.
```
## ğŸ§  How It Works
One-hot encoding of names as sequence of character vectors.

At each time step t:

hidden_t = tanh(UÂ·x_t + WÂ·hidden_{t-1})

output_t = softmax(VÂ·hidden_t)

Loss: Sum of cross-entropy across time steps.

BPTT: Gradients are computed by unrolling through time and applying chain rule.

Parameter updates: Performed via SGD with optional learning rate decay.

## ğŸ“Š Evaluation / Results
Training and validation loss decreased consistently.

Test accuracy ranges between 35â€“45% depending on data and training length.

Confusion matrix reveals common confusions (e.g. Spanish â†” Portuguese).

This simple RNN forms a solid foundation for further enhancements like LSTM, GRU, or using PyTorch.



